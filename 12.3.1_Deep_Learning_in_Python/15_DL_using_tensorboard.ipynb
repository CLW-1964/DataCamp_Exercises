{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO TENSORBOARD\n",
    "## The TensorBoard Visualization Framework\n",
    "\n",
    "\n",
    "<img src='images/tensorboard-the-loop-of-progress.jpg' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Change the backend to 'tensorflow' in /home/vagrant/.keras/keras.json__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 2000                                              \n",
    "max_len = 500                                                    \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128,\n",
    "                           input_length=max_len,\n",
    "                           name='embed'))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING THE MODEL WITH A TENSORBOARD CLASSBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 88s 4ms/step - loss: 0.6367 - acc: 0.6545 - val_loss: 0.4167 - val_acc: 0.8348\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 89s 4ms/step - loss: 0.4224 - acc: 0.8267 - val_loss: 0.4336 - val_acc: 0.8270\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 88s 4ms/step - loss: 0.3829 - acc: 0.8096 - val_loss: 0.4787 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 89s 4ms/step - loss: 0.3743 - acc: 0.7709 - val_loss: 0.5148 - val_acc: 0.7738\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.3213 - acc: 0.7449 - val_loss: 0.5959 - val_acc: 0.6718\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 95s 5ms/step - loss: 0.2817 - acc: 0.6844 - val_loss: 0.6403 - val_acc: 0.6476\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 88s 4ms/step - loss: 0.2536 - acc: 0.6417 - val_loss: 0.7022 - val_acc: 0.5958\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 86s 4ms/step - loss: 0.2341 - acc: 0.6052 - val_loss: 0.8881 - val_acc: 0.5086\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 86s 4ms/step - loss: 0.1843 - acc: 0.5441 - val_loss: 0.8974 - val_acc: 0.4848\n"
     ]
    }
   ],
   "source": [
    "# Log files will be written at this location.\n",
    "# Records activation histograms every 1 epoch\n",
    "# Records embedding data every 1 epoch\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='/home/vagrant/datascience/tf_log',                  \n",
    "        histogram_freq=1,                      \n",
    "        embeddings_freq=1,                     \n",
    "    )\n",
    "]\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! tensorboard --logdir=/home/vagrant/datascience/tf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then browse to http://localhost:6006 and look at your model training. In addition to live graphs of the training and validation metrics, you get access to the Histograms tab, where you can find pretty visualizations of histograms of activation values taken by your layers.\n",
    "\n",
    "<img src='./images/tf_metrics_monitoring.jpg' />\n",
    "\n",
    "<img src='./images/tf_activation_histograms.jpg' />\n",
    "\n",
    "The Embeddings tab gives you a way to inspect the embedding locations and spatial relationships of the 10,000 words in the input vocabulary, as learned by the initial Embedding layer. Because the embedding space is 128-dimensional, TensorBoard automatically reduces it to 2D or 3D using a dimensionality-reduction algorithm of your choice: either principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE). In the figure below, in the point cloud, you can clearly see two clusters: words with a positive connotation and words with a negative connotation. The visualization makes it immediately obvious that embeddings trained jointly with a specific objective result in models that are completely specific to the underlying task—that’s the reason using pretrained generic word embeddings is rarely a good idea.\n",
    "\n",
    "<img src='./images/3D_word_embeddings.jpg' />\n",
    "\n",
    "The Graphs tab shows an interactive visualization of the graph of low-level TensorFlow operations underlying your Keras model. As you can see, there’s a lot more going on than you would expect. The model you just built may look simple when defined in Keras—a small stack of basic layers—but under the hood, you need to construct a fairly complex graph structure to make it work. A lot of it is related to the gradient-descent process. This complexity differential between what you see and what you’re manipulating is the key motivation for using Keras as your way of building models, instead of working with raw TensorFlow to define everything from scratch. Keras makes your workflow dramatically simpler.\n",
    "\n",
    "<img src='./images/tf_graph_visualization.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMMARY\n",
    "\n",
    "Keras callbacks provide a simple way to monitor models during training and automatically take action based on the state of the model.\n",
    "\n",
    "When you’re using TensorFlow, TensorBoard is a great way to visualize model activity in your browser. You can use it in Keras models via the TensorBoard callback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
