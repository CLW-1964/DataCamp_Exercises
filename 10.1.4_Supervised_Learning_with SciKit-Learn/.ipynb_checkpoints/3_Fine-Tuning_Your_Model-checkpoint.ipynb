{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the PIMA Indians data and create X and y\n",
    "df = pd.read_csv('D:/Springboard_DataCamp/data/Supervised_Learning/diabetes.csv')\n",
    "X = df.drop('diabetes', axis=1)\n",
    "y = df.diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for classification\n",
    "In Chapter 1, you evaluated the performance of your k-NN classifier based on its accuracy. However, accuracy is not always an informative metric. In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report.\n",
    "\n",
    "The classification report consists of three rows, and an additional support column. The support gives the number of samples of the true response that lie in that class. The precision, recall, and f1-score columns, then, gave the respective metrics for that particular class.\n",
    "\n",
    "Here, you'll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. Therefore, it is a binary classification problem. A target value of **`0`** indicates that the patient **does not** have diabetes, while a value of **`1`** indicates that the patient **does** have diabetes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153  38]\n",
      " [ 51  66]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.80      0.77       191\n",
      "          1       0.63      0.56      0.60       117\n",
      "\n",
      "avg / total       0.71      0.71      0.71       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Precision`** can be seen as a measure of exactness, or quality. also called the **`positive predictive value`**\n",
    "\n",
    "- precision = *tp* / *(tp + fp)* for 0 = 153/(153+51), for 1 = 66/(66+38)\n",
    "\n",
    "**`Recall`** is a measure of completeness or quantity\n",
    "\n",
    "- recall = *tp* / *(tp + fn)* for 0 = 153/(153+38), for 1 = 66/(66+51)\n",
    "\n",
    "**`F1 score`** combines precision and recall\n",
    "\n",
    "- F1 score = 2 x (precision x recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a logistic regression model\n",
    "Time to build your first logistic regression model! scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as 'estimators'. You'll see this now for yourself as you train a logistic regression model on exactly the same data as in the previous exercise. Will it outperform k-NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[157  34]\n",
      " [ 53  64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.82      0.78       191\n",
      "          1       0.65      0.55      0.60       117\n",
      "\n",
      "avg / total       0.71      0.72      0.71       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting an ROC curve\n",
    "Classification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models. Most classifiers in scikit-learn have a **`.predict_proba()`** method which returns the probability of a given sample being in a particular class. \n",
    "\n",
    "Having built a logistic regression model, you'll now evaluate its performance by plotting an ROC curve. In doing so, you'll make use of the **`.predict_proba()`** method and become familiar with its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xec1NX1//HXcVGxEgU0Su+woBKzAakqIkVB0YgiBENcmgZEUBGjoPBTAgiCNGl2RaxEEvlKjNFoDEVCE1FkpQtKkSIWhOX8/pjZzbhumYX97OzMvJ+Pxz6cz8yd+ZwP4J65937uuebuiIiIABwX6wBERKTkUFIQEZFsSgoiIpJNSUFERLIpKYiISDYlBRERyaakICIi2ZQUJKGY2UYz+97MDpjZl2b2lJmdmqNNUzP7p5l9Y2b7zOyvZpaao83pZjbBzDaHPysjfFwuj/Oamd1mZqvN7Fsz22pmL5vZeUFer0hRU1KQRNTR3U8FGgK/Au7JesHMmgB/B14HzgWqASuBD8yserjNCcDbQH2gHXA60BTYDTTK45yPAgOA24AzgdrAX4ArCxu8mZUq7HtEioppRbMkEjPbCPR093+Ej8cA9d39yvDx+8BH7n5rjvf9H7DT3W8ys57AQ0ANdz8QxTlrAZ8CTdx9SR5t3gWec/dZ4eMe4Tibh48d6AfcDpQCFgAH3P3OiM94HfiXuz9iZucCk4CWwAFgvLtPjOKPSCRf6ilIwjKzikB7ICN8fDKhb/wv59L8JeDy8OPWwJvRJISwy4CteSWEQugENAZSgdnADWZmAGZ2BtAGmGNmxwF/JdTDqRA+/+1m1vYYzy+ipCAJ6S9m9g2wBdgB3B9+/kxC/+a35/Ke7UDWfEHZPNrkpbDt8/Jnd//a3b8H3gccaBF+7TpgobtvA34DlHf3Ee7+o7uvB2YCXYogBklySgqSiDq5+2nAJUBd/vfLfg9wBDgnl/ecA+wKP96dR5u8FLZ9XrZkPfDQuO4c4MbwU12B58OPqwDnmtnerB/gT8DZRRCDJDklBUlY7v4v4ClgbPj4W2Ah0DmX5tcTmlwG+AfQ1sxOifJUbwMVzSwtnzbfAidHHP8yt5BzHL8AXGdmVQgNK70afn4LsMHdfxHxc5q7XxFlvCJ5UlKQRDcBuNzMGoaPhwC/D98+epqZnWFmDwJNgOHhNs8S+sX7qpnVNbPjzKysmf3JzH72i9fd1wFTgRfM7BIzO8HMSptZFzMbEm62ArjWzE42s5pAekGBu/tyYCcwC1jg7nvDLy0B9pvZ3WZ2kpmlmFkDM/vN0fwBiURSUpCE5u47gWeAoeHjfwNtgWsJzQNsInTbavPwL3fc/SChyeZPgbeA/YR+EZcDFudxqtuAycAUYC/wOXANoQlhgPHAj8BXwNP8byioIC+EY5kdcU2ZQEdCt9xuIDTsNQsoE+VniuRJt6SKiEg29RRERCSbkoKIiGRTUhARkWxKCiIiki3uCm+VK1fOq1atGuswRETiyn//+99d7l6+oHZxlxSqVq3K0qVLYx2GiEhcMbNN0bTT8JGIiGRTUhARkWxKCiIikk1JQUREsikpiIhItsCSgpk9YWY7zGx1Hq+bmU0Mb4i+yswuDCoWERGJTpA9hacIbXqel/ZArfBPb+CxAGMREZEoBLZOwd3fM7Oq+TS5GngmvMPUIjP7hZmd4+5Fsa2hiEhgZi/ezOsrvii28x05ksmPPx7iwupncX/H+oGeK5ZzChWI2H4Q2Bp+7mfMrLeZLTWzpTt37iyW4ERE8vL6ii9Ys31/sZxr7969fPjhUj7++GOKY6uDWK5otlyey/WK3X0GMAMgLS1NG0CISJErzLf/Ndv3k3rO6bzYp0lg8ezdu5e77rqLl2bNombNmsyaNYuLL24Q2PmyxDIpbAUqRRxXBLbFKBYRSXJZ3/5Tzzm9wLap55zO1Q1zHdgoEpmZmTRt2pS1a9cyePBgHnjgAU466aTAzhcplklhHtDPzOYQ2pR8n+YTRJJLcY/N56c4vv0XZPfu3Zx55pmkpKTw0EMPUalSJdLS0oo1hsCSgpm9AFwClDOzrcD9wPEA7j4NmA9cAWQA3wF/CCoWEYmNgn7pL97wNQCNq51ZXCHlKehv//lxd55//nkGDBjAqFGj6NWrF9dcc01MYgny7qMbC3jdgT8GdX4Rib2ChmQaVzuTqxtWoGvjysUcWcmxZcsW+vbty/z587noooto1qxZTOOJu9LZIhK8ohrWKQlDMiXZCy+8QJ8+fcjMzGTChAn069ePlJSUmMakMhci8jNFdctlLIdk4sEZZ5xB48aNWb16NQMGDIh5QgD1FEQkh9mLN7N4w9c0rnamvuEXscOHDzN+/Hh+/PFH7r33Xtq1a0fbtm0xy+0O/dhQT0FEfiJr2Ejf8IvWypUrueiiixg8eDCrVq3KXohWkhICKCmISC4aVzszqSd/i9LBgwcZOnQoaWlpbNmyhZdffpk5c+aUuGSQRUlBRCRA69atY/To0XTt2pU1a9Zw3XXXldiEAJpTEBEpcgcOHOD111+nW7duNGjQgE8//ZTq1avHOqyoKCmIJKH8bjmNttSD5O6tt96id+/ebNq0iQsvvJB69erFTUIAJQWRhBPNGoP8VhLrNtKjs2fPHu68806eeOIJateuzb/+9S/q1asX67AKTUlBJMFEU9hNK4mLVmZmJs2aNeOzzz7jnnvuYdiwYZQuXTrWYR0VJQWRBBDZO9Aq4uKza9eu7AJ2I0eOpHLlylx4YXzvLKy7j0QSQOQKZA3/BM/deeaZZ6hduzazZs0CoFOnTnGfEEA9BZG4Ec3ksHoHwdu0aRN9+vRhwYIFNG3alJYtW8Y6pCKlnoJInMivHpF6B8Xjueeeo0GDBvz73/9m0qRJvP/++9StWzfWYRUp9RREikFRVB1VbyD2ypcvT7NmzZg+fTpVqlSJdTiBUFIQCUDOJFAUm8moN1D8Dh06xLhx4zh06BBDhw6lbdu2tGnTpkSvSD5WSgoiAch5W6huAY0/y5cvJz09neXLl9OlSxfcHTNL6IQASgoiR6Wg4SAN9cSvH374gREjRjBmzBjKlSvHq6++yrXXXhvrsIqNJppFjkJBm9BoqCd+ZWRkMHbsWG666SY++eSTpEoIoJ6CSJ50C2jyOHDgAHPnzqV79+40aNCAtWvXUq1atViHFRNKCiJhhZkcVk8gcSxYsIDevXuzZcsW0tLSqFevXtImBFBSEMmmyeHksnv3bgYNGsQzzzxD3bp1ef/99+OygF1RU1KQpJWzZ6AhoeSRVcAuIyODe++9l/vuuy9uC9gVNSUFSVo5ewYaEkp8O3fupGzZsqSkpDB69GiqVKlCw4YNYx1WiaKkIEkht0lj9QySh7vz1FNPMWjQIEaNGkWfPn24+uqrYx1WiaSkIHEt2vIRuU0aq2eQHDZu3Ejv3r156623aNGiBZdeemmsQyrRlBQkrkWzoQxo0jhZPfvss9xyyy2YGVOnTqVPnz4cd5yWZ+VHSUHijjaUkWidffbZtGzZkmnTplG5sr4QRENJQeJOZO9AQ0AS6dChQ4wZM4bMzEyGDRtGmzZtaNOmTazDiitKChI3snoI6h1IbpYtW8bNN9/MypUr6dq1a3YBOykcJQUp0SKHiiIni9U7kCzff/89w4cPZ+zYsZQvX565c+fSqVOnWIcVtwJNCmbWDngUSAFmufuoHK9XBp4GfhFuM8Td5wcZk8SXyJ6BJoslN+vXr+eRRx6hR48ePPzww5xxxhmxDimuBZYUzCwFmAJcDmwFPjSzee6+JqLZfcBL7v6YmaUC84GqQcUkJUs0t5NqqEhys3//fl577TV69OhB/fr1WbduXcLuhFbcgrw3qxGQ4e7r3f1HYA6Qc7WIA1n3EpYBtgUYj5QwBZWfBq0lkJ+bP38+DRo0ID09nU8++QRACaEIBTl8VAHYEnG8FWico80DwN/NrD9wCtA6tw8ys95Ab0C3lcUx1RqSY7Fr1y4GDhzIc889R2pqKh988IEK2AUgyJ5CbtP+nuP4RuApd68IXAE8a2Y/i8ndZ7h7mrunlS9fPoBQJWizF2/mT3M/yp4sBvUCJHpZBezmzJnDsGHDWLZsGRdddFGsw0pIQfYUtgKVIo4r8vPhoXSgHYC7LzSz0kA5YEeAcUkMZPUQRl5zniaKJWpfffUV5cuXJyUlhbFjx1KlShXOP//8WIeV0ILsKXwI1DKzamZ2AtAFmJejzWbgMgAzqweUBnYGGJPEUONqZyohSFTcnccff5w6deowY8YMADp27KiEUAwCSwrufhjoBywAPiF0l9HHZjbCzK4KN7sD6GVmK4EXgB7unnOISUSSyPr162ndujU9e/akYcOGtG6d61SjBCTQdQrhNQfzczw3LOLxGqBZkDGISPx4+umnufXWW0lJSWHatGn06tVLBeyKmVY0i0iJce6559KqVSsee+wxKlasGOtwkpKSggQmt2qmIpF+/PFHRo0axZEjR3jggQe4/PLLufzyy2MdVlJTv0wCE7k4TbefSk4ffvghv/71r7n//vtZv349mk4sGdRTkEBpcZrk9N133zFs2DDGjx/POeecw7x58+jYsWOsw5IwJQU5JvnVL9KQkeRmw4YNTJo0iV69ejF69GjKlCkT65AkgoaP5JjkV79IQ0aSZd++fTz55JMA1K9fn4yMDKZNm6aEUAKppyDHTENEkp833niDPn36sH37dpo0aULdunWpVKlSwW+UmFBPQY7a7MWbf1LLSCTSzp076datGx06dOCMM85g4cKF1K1bN9ZhSQHUU5CjljWXoCEiySkzM5PmzZuzYcMGhg8fzpAhQzjhhBNiHZZEIaqkEK5dVNndMwKOR+JA5F7Jqmckkb788kvOOussUlJSGDduHFWrVqVBgwaxDksKocDhIzO7EvgIeCt83NDM5gYdmJRckVtkqpcgAEeOHGH69OnUrl2b6dOnA9ChQwclhDgUTU9hBKHNcd4BcPcVZlYz0KikxNPksmTJyMigV69evPvuu7Rq1Yq2bdvGOiQ5BtFMNB9y9705ntPSwySlyWWJ9OSTT3LeeeexbNkyZs6cyT/+8Q+qV68e67DkGETTU/jEzK4HjjOzasAAYFGwYUlJpclliVS5cmXatm3LlClTqFBB/yYSgRVUb8TMTgGGAW3CTy0Ahrv79wHHlqu0tDRfunRpLE6d1CInlzV0lLwOHjzIn//8Z44cOcKIESNiHY4Ugpn9193TCmoXzfBRW3e/291/Ff4ZArQ/9hAlnmhyWRYvXsyvf/1rhg8fzubNm1XALkFFM3x0H/BajufuzeU5SQB51TJSDyF5ffvttwwdOpQJEyZQoUIF/va3v3HllVfGOiwJSJ5JwczaAu2ACmb2SMRLpwNHgg5MYiOyRxBJPYTktWnTJqZOnUrfvn0ZNWoUp5+uIoeJLL+ewg5gNfAD8HHE898AQ4IMSmJLPQLZu3cvr7zyCj179iQ1NZWMjAzthJYk8kwK7r4cWG5mz7v7D8UYk4jE0Ouvv84tt9zCjh07aN68OXXr1lVCSCLRTDRXMLM5ZrbKzD7L+gk8MhEpVjt27KBLly506tSJ8uXLs2jRIhWwS0LRTDQ/BTwIjCV019Ef0JyCSELJzMykWbNmbN68mQcffJDBgwdz/PHHxzosiYFoksLJ7r7AzMa6++fAfWb2ftCBiUjwtm3bxi9/+UtSUlJ49NFHqVq1KqmpqbEOS2IomuGjg2ZmwOdm1tfMOgJnBRyXFLPZizdzw/SFee6iJonlyJEjPPbYY9StW5dp06YBcMUVVyghSFQ9hYHAqcBtwENAGeDmIIOS4hG5JiGrnlHjamfq1tME99lnn9GrVy/ee+89WrduTfv2Wosq/1NgUnD3xeGH3wDdAcxMtyIkgMg1CVnJQHsjJLbHH3+cfv36Ubp0aZ544gl69OhBaCBAJCTfpGBmvwEqAP92911mVh+4G2gFKDHEocjegVYpJ5+qVavSvn17pkyZwjnnnBPrcKQEynNOwcz+DDwPdAPeNLN7Ce2psBKoXTzhSVHL6h2AVikng4MHD3Lfffdx3333AXDZZZfx2muvKSFInvLrKVwNXODu35vZmcC28PHa4glNipKqnCaf//znP6Snp/Ppp59y88034+4aKpIC5Xf30Q9Z5bHd/WvgUyWE+KUqp8njwIEDDBgwgObNm/Pdd9/x5ptv8vjjjyshSFTy6ylUN7OsSqgGVI04xt2vLejDzawd8CiQAsxy91G5tLkeeIDQbm4r3b1r9OFLYaiHkBw2b97M9OnT+eMf/8jIkSM57bTTYh2SxJH8ksJvcxxPLswHm1kKMAW4HNgKfGhm89x9TUSbWsA9QDN332NmWv9QhHKbVJbEtGfPHl5++WV69+5Namoq69ev59xzz411WBKH8iuI9/YxfnYjIMPd1wOY2RxC8xRrItr0Aqa4+57wOXcc4zklQuSQkYaNEtfcuXO59dZb2blzJxdffDF16tRRQpCjFs3itaNVAdgScbwVaJyjTW0AM/uA0BDTA+7+Zs4PMrPeQG8I7Qkr0dOQUeL68ssv6d+/P6+88goNGzbkjTfeoE6dOrEOS+JckEkht1mtnPv3lQJqAZcQWvfwvpk1cPe9P3mT+wxgBoT2aC76UBNLzjuNJPFkZmbSokULtmzZwsiRI7nzzjtVwE6KRNRJwcxOdPeDhfjsrUCliOOKhG5rzdlmkbsfAjaY2VpCSeLDQpxHctCdRolr69atnHvuuaSkpDBx4kSqVaum8tZSpAosiGdmjczsI2Bd+PgCM5sUxWd/CNQys2pmdgLQBZiXo81fgEvDn1uO0HDS+kLEL3nIGjZS2YrEcOTIESZNmkTdunV57LHHAGjfvr0SghS5aKqkTgQ6ALsB3H0l4V/k+XH3w0A/YAHwCfCSu39sZiPM7KpwswXAbjNbQ2i19F3uvrvwlyGSuD799FNatmzJbbfdRvPmzenQoUOsQ5IEFs3w0XHuvinHwpfMaD7c3ecD83M8NyzisQODwj8iksOsWbPo168fJ598Mk8//TTdu3fXIjQJVDRJYYuZNQI8vPagP6DtOEWKQY0aNejYsSOTJ0/m7LPPjnU4kgSiSQq3EBpCqgx8Bfwj/JyIFLEffviBESNGADBy5EguvfRSLr20wNFakSITTVI47O5dAo9EjkrkquUsuhU1Pn3wwQekp6ezdu1aevbsqQJ2EhPRTDR/aGbzzez3ZqYiKiVMZCnsLLoVNb5888039O/fnxYtWnDw4EEWLFjAzJkzlRAkJqLZea2GmTUldEvpcDNbAcxx9zmBRye50kY5iWXr1q3MmjWL/v3789BDD3HqqafGOiRJYtH0FHD3/7j7bcCFwH5Cm+9IjGijnPi3e/fu7PUG9erVY/369Tz66KNKCBJzBfYUzOxUQoXsugD1gNeBpgHHJQVQ7yA+uTuvvvoqf/zjH/n6669p1aoVderU0U5oUmJE01NYDVwEjHH3mu5+h7svDjgukYSzfft2fvvb39K5c2cqVarE0qVLVcBOSpxo7j6q7u5HAo9EJIFlFbD74osvGDNmDAMHDqRUqSDrUYocnTz/VZrZOHe/A3jVzH5WmTSandfk2OR2uynoltN4smXLFipUqEBKSgpTpkyhWrVq1K5dO9ZhieQpv+GjF8P/nUxoB7WcPxKw3G43BU0ux4PMzEwmTpz4kwJ2bdu2VUKQEi+/ndeWhB/Wc/efbMVpZv2AY92ZTaKgCeX488knn5Cens7ChQtp3749HTt2jHVIIlGLZqL55lyeSy/qQCRk9uLN3DB9ITdMX5hrL0FKthkzZtCwYUM+++wznn32Wd544w3tFihxJb85hRsI3YZazcxei3jpNGBv7u+SY6V9leNbrVq1uOaaa5g4cSJnnXVWrMMRKbT8bn9YQmgPhYr8dA7hG2B5kEElo5xbaGrIKD58//33PPDAA5gZo0aNUgE7iXv5zSlsADYQqooqAdMWmvHnvffeo2fPnqxbt46+ffuqgJ0khPyGj/7l7heb2R4g8pZUI7Q/zpmBR5dk1EOID/v372fIkCE89thjVK9enbfffptWrVrFOiyRIpHf8FFWH7hccQQiEi+2bdvGU089xaBBgxgxYgSnnHJKrEMSKTJ53n0UsYq5EpDi7plAE6APoP8LJKns2rWLqVOnAlC3bl02bNjAuHHjlBAk4URzS+pfCG3FWQN4hlBRvNmBRiVSQrg7L774Iqmpqdx+++189lloJ1ptjSmJKpqkcMTdDwHXAhPcvT+gmVBJeNu2baNTp0506dKFKlWq8N///lcrkiXhRbUdp5l1BroDncLPHR9cSCKxl5mZScuWLfniiy8YO3YsAwYMUAE7SQrR/Cu/GbiVUOns9WZWDXgh2LAST17F7bKoyF3JsGnTJipWrEhKSgpTp06levXq1KxZM9ZhiRSbAoeP3H01cBuw1MzqAlvc/aHAI0sweRW3y6L1CbGVmZnJI488Qr169bIL2LVp00YJQZJONDuvtQCeBb4gtEbhl2bW3d0/CDq4RKN1CCXT6tWrSU9PZ8mSJXTo0IFOnToV/CaRBBXN8NF44Ap3XwNgZvUIJYm0IAMTKQ7Tpk3jtttuo0yZMsyePZsuXbpoVbIktWiSwglZCQHA3T8xsxMCjCnu5TZ/oDmDkiWrJEW9evXo3LkzEyZMoHz58rEOSyTmokkKy8xsOqHeAUA3VBAvX5F1jLJozqBk+O677xg2bBgpKSmMHj2aiy++mIsvvjjWYYmUGNEkhb6EJpoHE5pTeA+YFGRQiUDzByXPu+++S8+ePfn888+59dZbVcBOJBf5JgUzOw+oAcx19zHFE5JI0dq3bx+DBw9mxowZ1KhRg3/+858qby2ShzxvSTWzPxEqcdENeMvMctuBTaTE2759O8899xx33nknq1atUkIQyUd+6xS6Aee7e2fgN8Athf1wM2tnZmvNLMPMhuTT7jozczPTHU1SJHbu3MmkSaFRzrp167Jx40YefvhhTj755BhHJlKy5ZcUDrr7twDuvrOAtj9jZimEdmxrD6QCN5pZai7tTiM0Z7G4MJ8vkht3Z/bs2dSrV4877rgju4Cd7iwSiU5+v+irm9lr4Z+5QI2I49fyeV+WRkCGu6939x+BOcDVubT7f8AY4IdCRy8SYcuWLXTs2JFu3bpRs2ZNli9frgJ2IoWU30Tzb3McTy7kZ1cAtkQcbwUaRzYws18Bldz9b2Z2Z14fZGa9gd4AlStXLmQYxWv24s0s3vA1jatpY7ridPjwYS655BK+/PJLxo8fT//+/UlJSYl1WCJxJ789mt8+xs/O7V6/7G09zew4QqulexT0Qe4+A5gBkJaW5gU0j6msRWtak1A8Nm7cSKVKlShVqhTTp0+nevXqVK9ePdZhicStIGsBbyW0a1uWisC2iOPTgAbAu+F7xX8JzDOzq9x9aYBxHbP8Kp6u2b6fxtXOpGvjkt2jiXeHDx9mwoQJDB06lDFjxtC/f39at24d67BE4l6hJo8L6UOglplVC5fF6ALMy3rR3fe5ezl3r+ruVYFFQIlPCJB/xVOtXA7eqlWraNKkCXfddRdt27blt7/NOdIpIkcr6p6CmZ3o7gejbe/uh82sH7AASAGecPePzWwEsNTd5+X/CSWbVizHxtSpUxkwYABnnHEGL774Ip07d9aqZJEiFE3p7EbA40AZoLKZXQD0DG/LmS93nw/Mz/HcsDzaXhJNwJKcskpSNGjQgC5dujB+/HjKlSsX67BEEk40PYWJQAdCq5tx95VmpiWhUiy+/fZb7rvvPkqVKsXDDz9My5YtadmyZazDEklY0cwpHOfum3I8lxlEMCKR3n77bc477zwmTJjAwYMHcS/RN56JJIRoksKW8BCSm1mKmd0OfBZwXJLE9u7dS8+ePWndujWlSpXivffeY+LEiZo7ECkG0SSFW4BBQGXgK+AijqIOUqLIWpwmwfnqq6+YM2cOd999NytXrqRFixaxDkkkaRQ4p+DuOwjdTipocVpQshLBgAEDqFOnDhs3btREskgMRHP30UwiViJncffegUQUB7Q4rei4O88//zwDBgzgwIEDXHHFFdSqVUsJQSRGohk++gfwdvjnA+AsIOr1CiJ52bx5M1deeSXdu3enTp06rFixglq1asU6LJGkFs3w0YuRx2b2LPBWYBFJUsgqYLdjxw4mTpzIrbfeqgJ2IiXA0dQ+qgZUKepAJDmsX7+eKlWqUKpUKWbOnEmNGjWoWrVqrMMSkbACh4/MbI+ZfR3+2Uuol/Cn4EMrWWYv3swN0xfmWfNI8nf48GFGjx5NamoqU6ZMAeCyyy5TQhApYfLtKVjoxvALgKySoEc8SVcQZRXBU8G7wluxYgXp6eksW7aMa665hs6dO8c6JBHJQ75Jwd3dzOa6+6+LK6CSTEXwCm/y5MkMHDiQsmXL8sorr6iiqUgJF83dR0vM7MLAI5GEktWhPP/88+nWrRtr1qxRQhCJA3n2FMyslLsfBpoDvczsc+BbQjuqubsrUcjPHDhwgHvvvZfjjz+esWPHqoCdSJzJb/hoCXAh0KmYYpE49/e//53evXuzefNm+vfvn13uWkTiR35JwQDc/fNiikXi1J49exg0aBBPPfUUderU4b333qN58+axDktEjkJ+SaG8mQ3K60V3fySAeCQO7dixg1deeYV77rmHYcOGUbp06ViHJCJHKb+kkAKcSrjHIBLpyy+/5IUXXmDgwIHZBezKli0b67BE5BjllxS2u/uIYotE4oK788wzzzBw4EC+++47OnToQK1atZQQRBJEfrekqocgP7Fx40batWtHjx49SE1NVQE7kQSUX0/hsmKLooSavXhz9v4JWauZk9Xhw4e59NJL2bVrF1OmTKFv374cd1w0y1xEJJ7kmRTcPem3F4ssbZGs5S0yMjKoVq0apUqV4oknnqB69epUqaJ6iCKJSl/1CpBV2uLFPk2SamOdQ4cOMXLkSOrXr59dwO7SSy9VQhBJcEdTOlsS3LJly0hPT2fFihV07tyZG264IdYhiUgxUU9BfmLixIk0atSIL7/8ktdee42XXnqJs88+O9ZhiUgxUU8hLHJSOUsyTS5nlaTX1MXwAAAPjklEQVT41a9+xU033cS4ceM444wzYh2WiBQzJYWwyEnlLMkwufzNN99wzz33cOKJJzJu3DhatGhBixYtYh2WiMSIkkKEZNsv4c0336RPnz5s2bKF22+/XQXsRERzCslo9+7d/P73v6d9+/accsopfPDBBzzyyCNKCCKipJCMdu/ezdy5cxk6dCjLly+nSZPk6R2JSP4CTQpm1s7M1ppZhpkNyeX1QWa2xsxWmdnbZqab4AOyfft2xo4di7tTu3ZtNm3axIgRIzjxxBNjHZqIlCCBJQUzSwGmAO2BVOBGM0vN0Ww5kObu5wOvAGOCiidZuTtPPPEE9erVY+jQoWRkZADoziIRyVWQPYVGQIa7r3f3H4E5wNWRDdz9HXf/Lny4CKgYYDxJZ8OGDbRp04b09HQuuOACVq5cqQJ2IpKvIO8+qgBsiTjeCjTOp3068H+5vWBmvYHeAJUrJ0+piWNx+PBhWrVqxe7du3nsscfo3bu3CtiJSIGCTAq53criuTY0+x2QBlyc2+vuPgOYAZCWlpbrZ0jIunXrqF69OqVKleLJJ5+kRo0aVKpUKdZhiUicCPKr41Yg8rdRRWBbzkZm1hq4F7jK3Q8GGE9CO3ToEA8++CANGjRg8uTJAFxyySVKCCJSKEH2FD4EaplZNeALoAvQNbKBmf0KmA60c/cdAcaS0JYuXUp6ejqrVq2iS5cu3HjjjbEOSUTiVGA9BXc/DPQDFgCfAC+5+8dmNsLMrgo3e5jQPtAvm9kKM5sXVDyJ6tFHH6Vx48bs2rWL119/nRdeeIGzzjor1mGJSJwKtMyFu88H5ud4bljE49ZBnj+RZZWkSEtLIz09nTFjxvCLX/wi1mGJSJxT7aM4s3//fu6++25Kly7N+PHjadasGc2aNYt1WCKSIHSPYhyZP38+9evXZ8aMGZQqVQp33YglIkVLSSEO7Nq1i9/97ndceeWVlClThv/85z88/PDDKmAnIkVOSSEO7Nmzh7/+9a/cf//9LFu2jMaN81sDKCJy9DSnUEJ98cUXPP/889x1113UqlWLTZs2aSJZRAKnnkIJ4+7MnDmT1NRUHnjgAT7//HMAJQQRKRZJ3VOI3Je5JOzH/Pnnn9OrVy/eeecdLrnkEmbOnEnNmjVjGpOIJJekTgqR+zLHej/mw4cPc9lll/H1118zffp0evbsqQJ2IlLskjopQOz3ZV67di01atSgVKlSPP3009SoUYOKFVVBXERiQ19FY+THH39k+PDhnHfeeUyZMgWAiy++WAlBRGIq6XsKsbBkyRLS09NZvXo1Xbt2pVu3brEOSUQEUE+h2E2YMIEmTZpkrz14/vnnKVeuXKzDEhEBlBSKTVZJikaNGtGrVy8+/vhjOnToEOOoRER+SsNHAdu3bx+DBw/mpJNOYsKECTRt2pSmTZvGOiwRkVyppxCgv/71r6SmpjJr1ixOPPFEFbATkRJPSSEAO3fupGvXrlx11VWULVuWRYsWMXr0aBWwE5ESLymTwuzFm7lh+kLWbN8fyOfv27eP+fPnM3z4cJYuXcpvfvObQM4jIlLUknJOIXIlc1GtYt6yZQvPPfccQ4YMoWbNmmzatIkyZcoUyWeLiBSXpEsKsxdvZvGGr2lc7cwiWcl85MgRZsyYweDBg8nMzKRz587UrFlTCUFE4lLSDR9lFcArih7CunXraNWqFbfccguNGjXio48+UgE7EYlrSddTAGhc7Uy6Nq58TJ9x+PBhLr/8cvbu3cvjjz/OH/7wB00ki0jcS8qkcCw++eQTatWqRalSpXj22WepUaMG5557bqzDEhEpEkk3fHS0Dh48yP3338/555/P5MmTAWjRooUSgogkFPUUorBo0SLS09NZs2YN3bt3p3v37rEOSUQkEOopFGDcuHE0bdqUb775hvnz5/PMM89QtmzZWIclIhIIJYU8HDlyBIAmTZrQt29fVq9eTfv27WMclYhIsDR8lMPevXu54447OPnkk5k0aZIK2IlIUlFPIcJf/vIXUlNTefrppznttNNUwE5Eko6SArBjxw6uv/56rrnmGs4++2yWLFnCyJEjte5ARJKOkgKwf/9+3nrrLR566CGWLFnChRdeGOuQRERiImnnFDZv3syzzz7Ln/70J2rWrMnmzZs57bTTYh2WiEhMBdpTMLN2ZrbWzDLMbEgur59oZi+GX19sZlWDjCfLtm3bqF+/PiNHjuTzzz8HUEIQESHApGBmKcAUoD2QCtxoZqk5mqUDe9y9JjAeGB1UPABr165lxYoVrFu3jiZNmvDxxx+rgJ2ISIQgewqNgAx3X+/uPwJzgKtztLkaeDr8+BXgMgtodveB1z+i7ej5/FC6LHXq1GHBggVUrVo1iFOJiMStIOcUKgBbIo63Ao3zauPuh81sH1AW2BXZyMx6A70BKlc+uuqmdtxx1K1bj5NOOonOjarpziIRkVwEmRRy+62b88b/aNrg7jOAGQBpaWlHtXjg/o71oWP9o3mriEjSCHL4aCtQKeK4IrAtrzZmVgooA3wdYEwiIpKPIJPCh0AtM6tmZicAXYB5OdrMA34ffnwd8E/XMmIRkZgJbPgoPEfQD1gApABPuPvHZjYCWOru84DHgWfNLINQD6FLUPGIiEjBAl285u7zgfk5nhsW8fgHoHOQMYiISPRU5kJERLIpKYiISDYlBRERyaakICIi2Sze7gA1s53ApqN8ezlyrJZOArrm5KBrTg7Hcs1V3L18QY3iLikcCzNb6u5psY6jOOmak4OuOTkUxzVr+EhERLIpKYiISLZkSwozYh1ADOiak4OuOTkEfs1JNacgIiL5S7aegoiI5ENJQUREsiVkUjCzdma21swyzGxILq+faGYvhl9fbGZViz/KohXFNQ8yszVmtsrM3jazKrGIsygVdM0R7a4zMzezuL99MZprNrPrw3/XH5vZ7OKOsahF8W+7spm9Y2bLw/++r4hFnEXFzJ4wsx1mtjqP183MJob/PFaZ2YVFGoC7J9QPoTLdnwPVgROAlUBqjja3AtPCj7sAL8Y67mK45kuBk8OPb0mGaw63Ow14D1gEpMU67mL4e64FLAfOCB+fFeu4i+GaZwC3hB+nAhtjHfcxXnNL4EJgdR6vXwH8H6GdKy8CFhfl+ROxp9AIyHD39e7+IzAHuDpHm6uBp8OPXwEus/jetLnAa3b3d9z9u/DhIkI74cWzaP6eAf4fMAb4oTiDC0g019wLmOLuewDcfUcxx1jUorlmB04PPy7Dz3d4jCvu/h7570B5NfCMhywCfmFm5xTV+RMxKVQAtkQcbw0/l2sbdz8M7APKFkt0wYjmmiOlE/qmEc8KvGYz+xVQyd3/VpyBBSiav+faQG0z+8DMFplZu2KLLhjRXPMDwO/MbCuh/Vv6F09oMVPY/98LJdBNdmIkt2/8Oe+7jaZNPIn6eszsd0AacHGgEQUv32s2s+OA8UCP4gqoGETz91yK0BDSJYR6g++bWQN33xtwbEGJ5ppvBJ5y93Fm1oTQbo4N3P1I8OHFRKC/vxKxp7AVqBRxXJGfdyez25hZKUJdzvy6ayVdNNeMmbUG7gWucveDxRRbUAq65tOABsC7ZraR0NjrvDifbI723/br7n7I3TcAawkliXgVzTWnAy8BuPtCoDShwnGJKqr/349WIiaFD4FaZlbNzE4gNJE8L0ebecDvw4+vA/7p4RmcOFXgNYeHUqYTSgjxPs4MBVyzu+9z93LuXtXdqxKaR7nK3ZfGJtwiEc2/7b8QuqkAMytHaDhpfbFGWbSiuebNwGUAZlaPUFLYWaxRFq95wE3hu5AuAva5+/ai+vCEGz5y98Nm1g9YQOjOhSfc/WMzGwEsdfd5wOOEupgZhHoIXWIX8bGL8pofBk4FXg7PqW9296tiFvQxivKaE0qU17wAaGNma4BM4C533x27qI9NlNd8BzDTzAYSGkbpEc9f8szsBULDf+XC8yT3A8cDuPs0QvMmVwAZwHfAH4r0/HH8ZyciIkUsEYePRETkKCkpiIhINiUFERHJpqQgIiLZlBRERCSbkoKUOGaWaWYrIn6q5tO2al7VJAt5znfDlThXhktE1DmKz+hrZjeFH/cws3MjXptlZqlFHOeHZtYwivfcbmYnH+u5JTkoKUhJ9L27N4z42VhM5+3m7hcQKpb4cGHf7O7T3P2Z8GEP4NyI13q6+5oiifJ/cU4lujhvB5QUJCpKChIXwj2C981sWfinaS5t6pvZknDvYpWZ1Qo//7uI56ebWUoBp3sPqBl+72XhOv0fhevcnxh+fpT9b3+KseHnHjCzO83sOkL1pZ4Pn/Ok8Df8NDO7xczGRMTcw8wmHWWcC4kohGZmj5nZUgvtozA8/NxthJLTO2b2Tvi5Nma2MPzn+LKZnVrAeSSJKClISXRSxNDR3PBzO4DL3f1C4AZgYi7v6ws86u4NCf1S3houe3AD0Cz8fCbQrYDzdwQ+MrPSwFPADe5+HqEKALeY2ZnANUB9dz8feDDyze7+CrCU0Df6hu7+fcTLrwDXRhzfALx4lHG2I1TWIsu97p4GnA9cbGbnu/tEQnVxLnX3S8OlL+4DWof/LJcCgwo4jySRhCtzIQnh+/AvxkjHA5PDY+iZhGr65LQQuNfMKgKvufs6M7sM+DXwYbi8x0mEEkxunjez74GNhMov1wE2uPtn4defBv4ITCa0P8MsM3sDiLo0t7vvNLP14Zo168Ln+CD8uYWJ8xRCZR8id9263sx6E/r/+hxCG86syvHei8LPfxA+zwmE/txEACUFiR8Dga+ACwj1cH+2aY67zzazxcCVwAIz60mozPDT7n5PFOfoFlkwz8xy3WMjXI+nEaEibF2AfkCrQlzLi8D1wKfAXHd3C/2GjjpOQjuQjQKmANeaWTXgTuA37r7HzJ4iVBguJwPecvcbCxGvJBENH0m8KANsD9fI707oW/JPmFl1YH14yGQeoWGUt4HrzOyscJszLfr9qT8FqppZzfBxd+Bf4TH4Mu4+n9Akbm53AH1DqHx3bl4DOhHaB+DF8HOFitPdDxEaBrooPPR0OvAtsM/Mzgba5xHLIqBZ1jWZ2clmlluvS5KUkoLEi6nA781sEaGho29zaXMDsNrMVgB1CW1ZuIbQL8+/m9kq4C1CQysFcvcfCFWgfNnMPgKOANMI/YL9W/jz/kWoF5PTU8C0rInmHJ+7B1gDVHH3JeHnCh1neK5iHHCnu68ktDfzx8AThIaksswA/s/M3nH3nYTujHohfJ5FhP6sRABVSRURkQjqKYiISDYlBRERyaakICIi2ZQUREQkm5KCiIhkU1IQEZFsSgoiIpLt/wOi++9keppEmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the curve comes in play if you want to compare different methods that try to discriminate between two classes,\n",
    "\n",
    "You can construct the ROC curve for all these models and the one with the highest area under the curve can be seen as the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC computation\n",
    "Say you have a binary classifier that in fact is just randomly making guesses. It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. \n",
    "\n",
    "The Area under this ROC curve would be 0.5. This is one way in which the AUC is an informative metric to evaluate a model. If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!\n",
    "\n",
    "In this exercise, you'll calculate AUC scores using the roc_auc_score() function from **`sklearn.metrics`** as well as by performing cross-validation on the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7916051371548755\n",
      "AUC scores computed using 5-fold cross-validation: [0.74944444 0.80037037 0.8637037  0.83754717 0.87301887]\n",
      "5-fold cross val AUC scores:  [0.74944444 0.80037037 0.8637037  0.83754717 0.87301887]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Compute cross-validated AUC scores: cv_auc\n",
    "cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n",
    "print('5-fold cross val AUC scores: ', cv_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with GridSearchCV\n",
    "Hugo demonstrated how to tune the n_neighbors parameter of the KNeighborsClassifier() using GridSearchCV on the voting dataset. You will now practice this yourself, but by using logistic regression on the diabetes dataset instead!\n",
    "\n",
    "Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: *`C`*. *`C`* controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large *`C`* can lead to an overfit model, while a small *`C`* can lead to an underfit model.\n",
    "\n",
    "The hyperparameter space for *`C`* has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal *`C`* in this hyperparameter space. The feature array is available as **`X`** and target variable array is available as **`y`**.\n",
    "\n",
    "You may be wondering why you aren't asked to split the data into training and test sets. Here, we want you to focus on the process of setting up the hyperparameter grid and performing grid-search cross-validation. In practice, you will indeed want to hold out a portion of your data for evaluation purposes, and you will learn all about this in the next video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 3.727593720314938}\n",
      "Best score is 0.70703125\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with RandomizedSearchCV\n",
    "GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. \n",
    "\n",
    "A solution to this is to use **`RandomizedSearchCV`**, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You'll practice using **`RandomizedSearchCV`** in this exercise and see how this works.\n",
    "\n",
    "Here, you'll also be introduced to a new model: the **Decision Tree**. Don't worry about the specifics of how this model works. Just like k-NN, linear regression, and logistic regression, decision trees in scikit-learn have **`.fit()`** and **`.predict()`** methods that you can use in exactly the same way as before. \n",
    "\n",
    "Decision trees have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for **`RandomizedSearchCV`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 6}\n",
      "Best score is 0.6575520833333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X,y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out set in practice I: Classification\n",
    "You will now practice evaluating a model with tuned hyperparameters on a hold-out set. \n",
    "\n",
    "In addition to *C*, logistic regression has a **`'penalty'`** hyperparameter which specifies whether to use **`'l1'`** or **`'l2'`** regularization. Your job in this exercise is to create a hold-out set, tune the **`'C'`** and **`'penalty'`** hyperparameters of a logistic regression classifier using **`GridSearchCV`** on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 31.622776601683793, 'penalty': 'l1'}\n",
      "Tuned Logistic Regression Accuracy: 0.7913043478260869\n"
     ]
    }
   ],
   "source": [
    "# Create the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg,param_grid, cv=5 )\n",
    "\n",
    "# Fit it to the training data\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out set in practice II: Regression\n",
    "Remember lasso and ridge regression from the previous chapter? Lasso used the **L1** penalty to regularize, while ridge used the **L2** penalty. There is another type of regularized regression known as the **`elastic net`**. In elastic net regularization, the penalty term is a linear combination of the **L1** and **L2** penalties:\n",
    "\n",
    "**`a∗L1+b∗L2`**\n",
    "\n",
    "In scikit-learn, this term is represented by the **`'l1_ratio'`** parameter: An **`'l1_ratio'`** of 1 corresponds to an **L1** penalty, and anything lower is a combination of **L1** and **L2**.\n",
    "\n",
    "In this exercise, you will **`GridSearchCV`** to tune the **`'l1_ratio'`** of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sixsi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\sixsi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\sixsi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\sixsi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\sixsi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned ElasticNet l1 ratio: {'l1_ratio': 0.0}\n",
      "Tuned ElasticNet R squared: 0.24557275911845422\n",
      "Tuned ElasticNet MSE: 0.17771953061203302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sixsi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
